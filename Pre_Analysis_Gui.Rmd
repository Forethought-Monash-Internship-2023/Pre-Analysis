---
title: "Pre_Analysis_V1"
author: "GUI GAO & Chatpisut Magic Makornkhan"
date: "2023-08-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = F)
```

```{r}
library(readxl)
library(tidyverse)
library(tidytext)
library(stringr)
library(lubridate)
library(wordcloud2)
library(RColorBrewer)
library(tidytext)
library(patchwork)
library(janitor)
library(rsample)
library(tidymodels)
library(caret)
library(tidygraph)
library(ggraph)
```


```{r}
IG_Telco_posts_clean <- read.csv("clean_data/Instagram/IG_Telco_posts.csv")
IG_Telco_comments_clean <- read.csv("clean_data/Instagram/IG_Telco_comments.csv")
FB_Telco_posts_clean <- read.csv("clean_data/Facebook/FB_Telco_posts.csv")
FB_Telco_comments_clean <- read.csv("clean_data/Facebook/FB_Telco_comments.csv")
FB_postcomt <- read.csv("clean_data/Facebook/fb_telco_postcomments.csv")
IG_postcomt <- read.csv("clean_data/Instagram/ig_telco_postcomments.csv")
```


## Cleaning Facebook Dataset

```{r}
FB_postcomt$post_date <- as.Date(FB_postcomt$post_date)

FB_comt <- FB_postcomt %>%
  select(post_url, comment_content) %>%
  aggregate(comment_content ~ post_url, FUN = paste, collapse = " ")

FB_number <- FB_postcomt %>%
  select(post_url, likes) %>%
  group_by(post_url) %>%
  mutate(likes = sum(likes))

FB_number[!duplicated(FB_number$post_url), ] -> FB_number
FB_postcomt[!duplicated(FB_postcomt$post_url), ] -> FB_postcomt

FB_postcomt <- FB_postcomt %>%
  select(!c(username, comment_content, likes, 
            comment_date, post_time_hr)) %>%
  left_join(FB_number, by = "post_url")

FB_postcomt <- FB_postcomt %>%
  left_join(FB_comt, by = "post_url")

FB_postcomt <- FB_postcomt %>%
  mutate(comment_content = ifelse(is.na(comment_content), "", comment_content))
```

## Cleaning Instagram Dataset

```{r}
IG_postcomt$numbers <- as.numeric(gsub(",", "", IG_postcomt$numbers))
IG_postcomt$post_date <- as.Date(IG_postcomt$post_date)

IG_comt <- IG_postcomt %>%
  select(post_url, content) %>%
  aggregate(content ~ post_url, FUN = paste, collapse = " ")

IG_number <- IG_postcomt %>%
  select(post_url, likes_number) %>%
  group_by(post_url) %>%
  mutate(likes_number = sum(likes_number))

IG_number[!duplicated(IG_number$post_url), ] -> IG_number
IG_postcomt[!duplicated(IG_postcomt$post_url), ] -> IG_postcomt

IG_postcomt <- IG_postcomt %>%
  select(!c(comment_username, content, date, 
            likes_number)) %>%
  left_join(IG_number, by = "post_url") %>%
  mutate(likes_number = ifelse(is.na(likes_number), 0, likes_number))

IG_postcomt <- IG_postcomt %>%
  left_join(IG_comt, by = "post_url")

IG_postcomt <- IG_postcomt %>%
  mutate(content = ifelse(is.na(content), "", content))
```


# Analysis

## IG_post wordcloud --- plan A --- Need!

```{r}
stopword <- get_stopwords(source = "snowball")

IG_postcomt <- IG_postcomt %>%
  mutate(post_caption = gsub('[^a-zA-Z|[:blank:]]', "", post_caption))

IG_post_word <- IG_postcomt %>%
  mutate(id = row_number()) %>%
  select(id, brand, post_caption, post_date) %>%
  unnest_tokens(output = word,
                input = post_caption,
                token = "words") %>%
  anti_join(stopword, by = "word") %>%
  filter(!str_detect(word, '^[[:digit:]]'))

colPalette <- c("#39A8AF", "#FECD03","#0060AE", "#0A8AD2", 
                "#e82f89", "#9b0082","#ef2d1e", "#FF5500")

IG_post_word %>%
  group_by(word) %>%
  summarise(freq = n()) %>%
  top_n(150) %>%
  wordcloud2(color = rep_len(colPalette, nrow(IG_post_word)),
             size = 0.6,
             shape = "circle")
```

## FB_post wordcloud --- plan A --- Need!

```{r}
FB_post_word <- FB_postcomt %>%
  mutate(id = row_number()) %>%
  select(id, brand, content, post_date, post_time) %>%
  unnest_tokens(output = word,
                input = content,
                token = "words") %>%
  anti_join(stopword, by = "word") %>%
  filter(!str_detect(word, '^[[:digit:]]'))

FB_post_word %>%
  group_by(word) %>%
  summarise(freq = n()) %>%
  top_n(150) %>%
  wordcloud2(color = rep_len(colPalette, nrow(FB_post_word)),
             size = 0.6,
             shape = "circle")
```

## IG_post wordcloud --- plan B--Need, but Magic will do some changes.

```{r}
library(wordcloud)
IG_post_word %>%
  filter(brand == "amaysim") %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = T) %>%
  reshape2::acast(word ~ sentiment, 
                  value.var = "n",
                  fill = 0) %>%
  comparison.cloud(colors = c("black", "#c30101"),
                   max.words = 150)
```

## IG_post wordcloud --- plan C --- No need.

```{r}
# %>% filter(brand == "Optus")

library(quanteda)
library(quanteda.textplots)
dfm(IG_post_word$word, remove_punct = TRUE) %>%
  dfm_trim(min_termfreq = 4) -> text_word

col <- sapply(seq(0.4, 1, 0.1), function(x) adjustcolor("#940000", x))
textplot_wordcloud(text_word,
                   adjust = 0.5,
                   min_size = 1,
                   random_order = F,
                   color = col,
                   rotation = F)
```

## IG_post_date Total Number Analysis --- No need.

```{r}
IG_post_wday <- IG_postcomt %>%
  mutate(numbers = ifelse(interactions == "likes", numbers, 0),
         post_wday = wday(post_date))

IG_post_wday %>%
  group_by(post_wday) %>%
  summarise(count = sum(numbers)) %>%
  ggplot(aes(x = post_wday,
             y = count)) +
  geom_area(alpha = 0.2) +
  stat_smooth(se = F,
              geom = "area",
              method = "loess",
              span = 0.8,
              alpha = 0.2,
              fill = "#940000") +
  scale_x_continuous("Day of Week",
                     labels = 1:7,
                     breaks = 1:7) +
  scale_y_continuous("Total Number of Likes",
                     labels = scales::comma) +
  theme_minimal()
```

## IG_post_date Number of Different Brands --- to do some changes add into R shiny.

```{r}
IG_post_wday %>%
  group_by(brand, post_wday) %>%
  summarise(count = sum(numbers)) %>%
  ggplot(aes(x = post_wday,
             y = count,
             fill = brand)) +
  geom_bar(show.legend = F,
           stat = "identity",
           alpha = 0.7) +
  scale_fill_manual(values = c("#FF5500", "#39A8AF", "#0060AE", "#ef2d1e")) +
  scale_x_continuous("Day of Week",
                     labels = 1:7,
                     breaks = 1:7) +
  scale_y_continuous("Number of Likes from Different Brands",
                     labels = scales::comma) +
  facet_wrap(~brand, scales = "free") +
  theme_minimal()
```

## FB_post_date Total Number Analysis -- No need.

```{r}
FB_post_wday <- FB_postcomt %>%
  mutate(post_wday = wday(post_date))

FB_post_wday %>%
  group_by(post_wday) %>%
  summarise(count = sum(reactions)) %>%
  ggplot(aes(x = post_wday,
             y = count)) +
  geom_area(alpha = 0.2) +
  stat_smooth(se = F,
              geom = "area",
              method = "loess",
              span = 0.8,
              alpha = 0.2,
              fill = "#940000") +
  scale_x_continuous("Day of Week",
                     labels = 1:7,
                     breaks = 1:7) +
  scale_y_continuous("Total Number of Likes",
                     labels = scales::comma) +
  theme_minimal()
```

## FB_post_date Number of Different Brands -- to do some changes add into R shiny.

```{r}
FB_post_wday %>%
  group_by(brand, post_wday) %>%
  summarise(count = sum(reactions)) %>%
  ggplot(aes(x = post_wday,
             y = count,
             fill = brand)) +
  geom_bar(show.legend = F,
           stat = "identity",
           alpha = 0.7) +
  scale_fill_manual(values = c("#FF5500", "#39A8AF", 
                               "#0060AE", "#ef2d1e")) +
  scale_x_continuous("Day of Week",
                     labels = 1:7,
                     breaks = 1:7) +
  scale_y_continuous("Number of Likes from Different Brands",
                     labels = scales::comma) +
  facet_wrap(~brand, scales = "free") +
  theme_minimal()
```

## IG_post Sentiment Analysis --- Need!

```{r}
IG_post_word %>%
  inner_join(get_sentiments("bing")) %>%
  mutate(Date = as.Date(post_date, format = "%m/%d/%Y"),
         Year = year(post_date),
         Month = month(post_date),
         Date = str_c(Year, "-",
                      Month, "-15"),
         Date = as.Date(Date)) %>%
  count(brand, index = Date, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  ggplot(aes(x = index,
             y = sentiment,
             fill = brand)) +
  geom_col(show.legend = F,
           alpha = 0.7) +
  scale_fill_manual(values = c("#FF5500", "#39A8AF", 
                               "#0060AE", "#ef2d1e")) +
  facet_wrap(~brand, ncol = 2,
             scales = "free_y") +
  theme_minimal() +
  labs(x = "Date")
```

## FB_post Sentiment Analysis --- Need!

```{r}
FB_post_word %>%
  inner_join(get_sentiments("bing")) %>%
  mutate(Hour = str_sub(post_time, 1, 2),
         Date = as.Date(post_date, format = "%m/%d/%Y"),
         Year = year(post_date),
         Month = month(post_date),
         Date = str_c(Year, "-",
                      Month, "-15"),
         Date = as.Date(Date)) %>%
  count(brand, index = Hour, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  ggplot(aes(x = index,
             y = sentiment,
             fill = brand)) +
  geom_col(show.legend = F,
           alpha = 0.7) +
  scale_fill_manual(values = c("#FF5500", "#39A8AF", 
                               "#0060AE", "#ef2d1e")) +
  facet_wrap(~brand, ncol = 2,
             scales = "free") +
  theme_minimal() +
  labs(x = "Hour")
```


## IG_commont Sentiment Analysis

### wordcloud--No need, use the planB.

```{r}
stopword <- get_stopwords(source = "snowball")

IG_com_word <- IG_Telco_posts_clean %>%
  inner_join(IG_Telco_comments_clean, by = "post_url") %>%
  mutate(id = row_number()) %>%
  select(id, brand, content, date) %>%
  unnest_tokens(output = word,
                input = content,
                token = "words") %>%
  anti_join(stopword, by = "word") %>%
  filter(!str_detect(word, '^[[:digit:]]'))

colPalette <- c("#39A8AF", "#FECD03","#0060AE", "#0A8AD2", 
                "#e82f89", "#9b0082","#ef2d1e", "#FF5500")

IG_com_word %>%
  inner_join(get_sentiments("bing")) %>%
  group_by(word) %>%
  summarise(freq = n()) %>%
  top_n(150) %>%
  wordcloud2(color = rep_len(colPalette, nrow(IG_com_word)),
             size = 0.6,
             shape = "circle")
```

### Coloumn plot about different sentiments in comments --- No Need.

```{r}
theme_lyrics <- function(aticks = element_blank(),
                         pgminor = element_blank(),
                         lt = element_blank(),
                         lp = "none")
{
  theme(plot.title = element_text(hjust = 0.5), #Center the title
        axis.ticks = aticks, #Set axis ticks to on or off
        panel.grid.minor = pgminor, #Turn the minor grid lines on or off
        legend.title = lt, #Turn the legend title on or off
        legend.position = lp) #Turn the legend on or off
}
```

```{r}
library(textdata)
IG_com_word %>%
  inner_join(get_sentiments("nrc")) %>%
  count(brand, sentiment) %>%
  mutate(sentiment = reorder(sentiment, n),
         brand = reorder(brand, n)) %>%
  ggplot(aes(x = sentiment,
             y = n,
             fill = sentiment)) +
  geom_col(alpha = 0.8) +
  facet_wrap(~brand, scales = "free_x") +
  theme_lyrics() +
  theme(panel.grid.major.x = element_blank(),
        axis.text.x = element_blank()) +
  labs(x = NULL, y = NULL) +
  ggtitle("Sentiment Analysis") +
  coord_flip()
```

### Donut plot about proportion of sentiments in comments --- Need
```{r}
library(webr)
IG_com_word %>%
  inner_join(get_sentiments("nrc")) %>%
  count(brand, sentiment) %>%
  PieDonut(aes(sentiment, count = n),
           labelposition = 1,
           r0 = 0.82)
```

### Bigram Network (basic processing, complete in R shiny code)--- Need!
```{r}
IG_com_bigrams <- IG_Telco_posts_clean %>%
  inner_join(IG_Telco_comments_clean, by = "post_url") %>%
  mutate(id = row_number()) %>%
  select(id, brand, content, date) %>%
  mutate(content = gsub('[^a-zA-Z|[:blank:]]', "", content)) %>%
  unnest_tokens(output = bigram,
                input = content,
                token = "ngrams",
                n = 2) %>%
  filter(!is.na(bigram)) %>%
  separate(bigram, c("word1", "word2"),
           sep = " ") %>%
  filter(!word1 %in% stopword$word) %>%
  filter(!word2 %in% stopword$word) %>%
  count(word1, word2, sort = T)
```


### tf-idf table --- Need!
```{r}
IG_com_word %>%
  anti_join(stopword) %>%
  inner_join(get_sentiments("bing")) %>%
  count(brand, word, sort = T) %>%
  bind_tf_idf(term = word,
              document = brand,
              n = n) %>%
  arrange(desc(tf_idf)) %>%
  filter(tf_idf >= 0.01) %>%
  mutate_at(vars(tf, idf, tf_idf), funs(round(., 5)))
# High value means that word has a high frequency within a document but is quite rare over all documents. If a word occurs in a lot of documents idf will be close to zero, so `tf_idf` value will be small.
```

## The histogram of themes -- Need!
```{r}
library(readxl)
FB_amaysim <- read_excel("result_for_histogram/Facebook/amaysim_fb_post_hist.xlsx")
FB_optus <- read_excel("result_for_histogram/Facebook/optus_fb_post_hist.xlsx")
FB_telstra <- read_excel("result_for_histogram/Facebook/telstra_fb_post_hist.xlsx")
FB_vodafoneAU <- read_excel("result_for_histogram/Facebook/vodafoneAU_fb_post_hist.xlsx")
IG_amaysim <- read_excel("result_for_histogram/Instagram/amaysim_ig_post_hist.xlsx")
IG_optus <- read_excel("result_for_histogram/Instagram/optus_ig_post_hist.xlsx")
IG_telstra <- read_excel("result_for_histogram/Instagram/telstra_ig_post_hist.xlsx")
IG_vodafoneAU <- read_excel("result_for_histogram/Instagram/vodafoneAU_ig_post_hist.xlsx")
```


## combine dataset
```{r}
data.frame(append(FB_amaysim, c(brand = "Amaysim"), after = 0)) -> FB_amaysim
data.frame(append(FB_optus, c(brand = "Optus"), after = 0)) -> FB_optus
data.frame(append(FB_telstra, c(brand = "Telstra"), after = 0)) -> FB_telstra
data.frame(append(FB_vodafoneAU, c(brand = "Vodafone AU"), after = 0)) -> FB_vodafoneAU
data.frame(append(IG_amaysim, c(brand = "Amaysim"), after = 0)) -> IG_amaysim
data.frame(append(IG_optus, c(brand = "Optus"), after = 0)) -> IG_optus
data.frame(append(IG_telstra, c(brand = "Telstra"), after = 0)) -> IG_telstra
data.frame(append(IG_vodafoneAU, c(brand = "Vodafone AU"), after = 0)) -> IG_vodafoneAU
FB_his <- rbind(FB_amaysim, FB_optus, FB_telstra, FB_vodafoneAU) %>%
  mutate(Themes = substr(Themes, 9, nchar(Themes)),
         Themes = gsub("_", " ", Themes))
IG_his <- rbind(IG_amaysim, IG_optus, IG_telstra, IG_vodafoneAU) %>%
  mutate(Themes = substr(Themes, 9, nchar(Themes)),
         Themes = gsub("_", " ", Themes))
```


```{r}
FB_his %>%
  mutate(Themes = reorder_within(Themes, Count, brand)) %>%
  ggplot(aes(x = Themes, y = Count, fill = brand)) +
  geom_col(show.legend = F,
               alpha = 0.7) +
  coord_flip() +
  facet_wrap(~ brand, scales = "free") +
  scale_fill_manual(values = c("#FF5500", "#39A8AF", 
                                   "#0060AE", "#ef2d1e")) +
  scale_x_reordered() +
  labs(x = NULL,
       y = NULL) +
  theme_minimal() +
  theme(text = element_text(size = 16))
```



# ----------- Content Analysis --------------- 
## Use wordcloud as the first step to get the idea of contents grouping
## Use Principal Components to group types of contents for posts and comments in order to categorize them into meaningful groups
## Find which groups related to other variables

# ----------- Date Time Analysis --------------- 
## Basic relationships of optimum and general date/time relationship with social media reactions

# ----------- Findings --------------- 
## Relationships of Date/Time to other variables and reactions
## Relationships of word cloud to reactions
## Search more on what client might want out of this
## Machine Learning on variables across others
## Brand auditing relative to time and their performance throughtout posts from past to recent
## Comparison between brands


## Try to use LDA to extract the themes of texts -- No working
```{r}
library(rtweet)
library(httr)
library(dplyr)
#library(qdap)
library(qdapRegex)
library(tm)
```

```{r}
test <- IG_Telco_posts_clean %>%
  select(post_caption) %>%
  mutate(post_caption = gsub("[^A-Za-z]", " ", post_caption))
```

```{r}
# create a corpus (transform tweets into documents)
corpus <- test %>%
  VectorSource() %>%
  Corpus()

# remove stop words, extra whitespace and transform to lower case
no_stop <- tm_map(corpus, removeWords, stopwords("english"))
lower <- tm_map(no_stop, tolower)
docs <- tm_map(lower, stripWhitespace)

# create a DTM
dtm <- DocumentTermMatrix(docs)
```

```{r}
# sum word counts in each document
row_terms <- apply(dtm , 1, sum)

# keep rows where sum is > 0
tweet_dtm <- dtm[row_terms > 0, ]
```

```{r}
# the lda function is within the topicmodels package
library(topicmodels)

lda3 <- LDA(tweet_dtm, 3)

# and check the results
terms_lda3 <- terms(lda3, 15)
terms_lda3
```

```{r}
lda_gibbs <- LDA(tweet_dtm, 
                 k = 3,
                 method = "Gibbs",
                 control = list(seed = 2)) %>%
  tidytext::tidy(matrix = "beta")

word_probs <- lda_gibbs %>%
  group_by(topic) %>%
  top_n(20, beta) %>%
  ungroup() %>%
  mutate(term_n = fct_reorder(term, beta))

ggplot(word_probs,
       aes(term_n,
           beta,
           fill = as.factor(topic)
           )
       ) +
geom_col(show.legend = FALSE) +
         coord_flip() +
         facet_wrap(~ topic, scales = "free")
```

